{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Call Center Speech Analytics Workflow in Python\n",
        "\n",
        "Chanin Nantasenamat, PhD\n",
        "\n",
        "[Data Professor YouTube channel](https://youtube.com/dataprofessor)\n",
        "\n",
        "> This tutorial demonstrates how to build a simple call center analytics tool in Python.\n",
        ">\n",
        "> You'll learn how to transcribe audio, identify speakers, analyze sentiment and perform data visualization of the call recording."
      ],
      "metadata": {
        "id": "r-gqfH47RUKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "Let's start by installing and importing the necessary libraries.\n"
      ],
      "metadata": {
        "id": "Tr9V3kJXRh2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install assemblyai"
      ],
      "metadata": {
        "id": "6O2kCQ7XShzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1f3X37cRRjd"
      },
      "outputs": [],
      "source": [
        "import assemblyai as aai\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Audio, Markdown, HTML\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load AssemblyAI API token\n",
        "\n",
        "First, let's load in the API token."
      ],
      "metadata": {
        "id": "N_vrGTg2SMCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "aai_key = userdata.get('AAI_KEY')"
      ],
      "metadata": {
        "id": "EsDmMLwAS6Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign the API token to the AssemblyAI SDK."
      ],
      "metadata": {
        "id": "bkX6wFtX9HZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aai.settings.api_key = aai_key"
      ],
      "metadata": {
        "id": "jx-fXIwpRrS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the Transcriber\n",
        "\n",
        "Let's instantiate the transcriber function so that we can transcribe the text from audio."
      ],
      "metadata": {
        "id": "U370hm_UTjTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcriber = aai.Transcriber()"
      ],
      "metadata": {
        "id": "DiqFxFKNTisB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Selection\n",
        "\n",
        "You can use a sample URL or upload your own audio file. Here, we provide options for both."
      ],
      "metadata": {
        "id": "NBeog8ELmEHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Use a sample call center audio from a URL\n",
        "audio_input = \"https://github.com/dataprofessor/assemblyai/raw/refs/heads/master/call-center-recording.wav\"\n",
        "\n",
        "# Option 2: Use a local file (uncomment and update path)\n",
        "# audio_input = \"./call-center-recording.wav\""
      ],
      "metadata": {
        "id": "LVexGMAFTvse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hear the audio\n",
        "display(Audio(audio_input))"
      ],
      "metadata": {
        "id": "1bKZXUEnmhx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process the Call Recording\n",
        "\n",
        "\n",
        "Let's transcribe the audio and specify the transcription configuration through the AssemblyAI SDK.\n"
      ],
      "metadata": {
        "id": "JPMh7Stqm6Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = aai.TranscriptionConfig(speaker_labels=True,\n",
        "                                 sentiment_analysis=True\n",
        "                                 )"
      ],
      "metadata": {
        "id": "YCfWhPBQmqtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate the `Transcriber()` function and apply it to transcribe text."
      ],
      "metadata": {
        "id": "6PY7qnalnHks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcriber = aai.Transcriber()\n",
        "transcript = transcriber.transcribe(audio_input, config)"
      ],
      "metadata": {
        "id": "oUlmDFr6nBXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript.audio_duration"
      ],
      "metadata": {
        "id": "by8SIdnNiT_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(transcript.words)"
      ],
      "metadata": {
        "id": "AsF37kGgiZV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speaker identification\n",
        "Process the transcript with speaker labels:"
      ],
      "metadata": {
        "id": "V7dQeiVqvClK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript.utterances"
      ],
      "metadata": {
        "id": "JmFKo3e5K2ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_speaker_labels = \"\"\n",
        "\n",
        "for utt in transcript.utterances:\n",
        "    text_with_speaker_labels += f\"Speaker {utt.speaker}: {utt.text}\\n\""
      ],
      "metadata": {
        "id": "eaZqm40bvCFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text_with_speaker_labels)"
      ],
      "metadata": {
        "id": "oLq-zl20vPTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Infer and count the number of unique speakers\n",
        "\n",
        "Count the unique speakers, then create a `LemurQuestion` for each speaker. Lastly, ask LeMUR the questions, specifying `text_with_speaker_labels` as the input_text."
      ],
      "metadata": {
        "id": "CgxwKUBwwTrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_speakers = set(utterance.speaker for utterance in transcript.utterances)\n",
        "\n",
        "questions = []\n",
        "for speaker in unique_speakers:\n",
        "    questions.append(\n",
        "        aai.LemurQuestion(\n",
        "        question=f\"Who is speaker {speaker}?\",\n",
        "        answer_format=\"<First Name> <Last Name (if applicable)>\")\n",
        "\n",
        "    )\n",
        "\n",
        "result = aai.Lemur().question(\n",
        "    questions,\n",
        "    input_text=text_with_speaker_labels,\n",
        "    final_model=aai.LemurModel.claude3_5_sonnet,\n",
        "    context=\"Your task is to infer the speaker's name from the speaker-labelled transcript\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "2NomAoe-vCIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.response"
      ],
      "metadata": {
        "id": "4g8t-AlhvCKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map speaker labels in transcript\n",
        "\n",
        "Here, we're ...\n",
        "- Identifying speakers\n",
        "  - By default, you've seen that different speakers were identified and assigned generic speaker labels of A and B.\n",
        "  - Here, we're asking the LeMUR LLM model to identify who is the speaker. Simply put, LLM helps us figure out the speaker names based on their mention in the transcript.\n",
        "- Mapping speaker labels in the transcript\n",
        "  - Speakers A and B labels were replaced with the identified speakers through mapping.\n",
        "  - `A = Michael Johnson` and `B = Sarah`"
      ],
      "metadata": {
        "id": "Z9eaTodqwOB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "speaker_mapping = {}\n",
        "\n",
        "for qa_response in result.response:\n",
        "    pattern = r\"Who is speaker (\\w)\\?\"\n",
        "    match = re.search(pattern, qa_response.question)\n",
        "    if match and match.group(1) not in speaker_mapping.keys():\n",
        "        speaker_mapping.update({match.group(1): qa_response.answer})"
      ],
      "metadata": {
        "id": "rTqas3aqvCNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "speaker_mapping"
      ],
      "metadata": {
        "id": "IIOAK2_tjiLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the Transcript with speaker names."
      ],
      "metadata": {
        "id": "WZoV3rI9wKs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for utterance in transcript.utterances:\n",
        "   speaker_name = speaker_mapping[utterance.speaker]\n",
        "   print(f\"{speaker_name}: {utterance.text}...\")"
      ],
      "metadata": {
        "id": "elrdIbvQwGD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we've just printed out the transcript with mapped speakers.\n",
        "\n",
        "Next, we're aggregating the transcript as a list so that we can save the mapped speakers."
      ],
      "metadata": {
        "id": "OI2cVOx8EzmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue_list = []\n",
        "\n",
        "for utterance in transcript.utterances:\n",
        "   speaker_name = speaker_mapping[utterance.speaker]\n",
        "   dialogue_list.append(f\"{speaker_name}: {utterance.text}\")\n",
        "\n",
        "dialogue_list"
      ],
      "metadata": {
        "id": "wQrWFInLtjN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entities visualization\n",
        "\n",
        "Named entities in text can be visualized using the `displacy()` function from the `spacy` library."
      ],
      "metadata": {
        "id": "tqi-j8FqBUco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll prepare the text by joining the dialogue into a string (the original data type is a list and is not compatible with the `displacy` function."
      ],
      "metadata": {
        "id": "TNia6TAxE6wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '\\n'.join(dialogue_list)"
      ],
      "metadata": {
        "id": "aPdIcunTE-3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "id": "KFiXLCTjj8OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we're visualizing the text in terms of the identified entities."
      ],
      "metadata": {
        "id": "8oBs1r0SE_VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the entities\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "geQOXJa4ApIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis\n",
        "\n",
        "Now, let's analyze the sentiment of the transcript and we can do that using the `sentiment_analysis` method. To use it, you can append it to the transcript like so `transcript.sentiment_analysis`."
      ],
      "metadata": {
        "id": "26uoUBPGMh4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript.sentiment_analysis"
      ],
      "metadata": {
        "id": "lQN7D47bAxzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's structure the data into a DataFrame"
      ],
      "metadata": {
        "id": "tdeDGIWrMBpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame of Speaker and Sentiment\n",
        "data = []\n",
        "index_value = 0  # Initialize an index counter\n",
        "\n",
        "for sentiment in transcript.sentiment_analysis:\n",
        "    # speaker = sentiment.speaker\n",
        "    speaker = speaker_mapping[sentiment.speaker]  # Applies our speaker mapping\n",
        "    sentiment_value = sentiment.sentiment.value\n",
        "    text = sentiment.text\n",
        "    data.append({'speaker': speaker, 'sentiment': sentiment_value, 'text': text, 'index': index_value})\n",
        "    index_value += 1  # Increment the index\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "metadata": {
        "id": "-DuoRLD3QecI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap of Sentiment Analysis 1"
      ],
      "metadata": {
        "id": "YiP6GAoGScrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we'll count the occurrences of each speaker-sentiment combination"
      ],
      "metadata": {
        "id": "wGSu1bPCMH_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each speaker-sentiment combination\n",
        "import altair as alt\n",
        "\n",
        "heatmap_data = df.groupby(['speaker', 'sentiment']).size().reset_index(name='count')\n",
        "\n",
        "font_size = 14\n",
        "\n",
        "# Create the base chart\n",
        "base = alt.Chart(heatmap_data).encode(\n",
        "    x=alt.X('speaker', axis=alt.Axis(title='Speaker', titleFontSize=font_size, labelFontSize=font_size)),\n",
        "    y=alt.Y('sentiment', axis=alt.Axis(title='Sentiment', titleFontSize=font_size, labelFontSize=font_size))\n",
        ")\n",
        "\n",
        "# Create the heatmap rectangles\n",
        "heatmap = base.mark_rect().encode(\n",
        "    color=alt.Color('count', title='Count', scale=alt.Scale(range='heatmap')),\n",
        "    tooltip=['speaker', 'sentiment', 'count']\n",
        ")\n",
        "\n",
        "# Add the text labels\n",
        "text = base.mark_text(fontSize=font_size, fontWeight='bold').encode(\n",
        "    text=alt.Text('count'),\n",
        "    color=alt.condition(\n",
        "        alt.datum.count > heatmap_data['count'].max() / 2,  # Adjust the threshold as needed\n",
        "        alt.value('white'),\n",
        "        alt.value('black')\n",
        "    )\n",
        ")\n",
        "\n",
        "# Combine the heatmap and text\n",
        "chart = (heatmap + text).properties(\n",
        "    # title='Sentiment by Speaker',\n",
        "    width=300,\n",
        "    height=300\n",
        ").interactive()"
      ],
      "metadata": {
        "id": "xOfGmlW8QyIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the structured data, we'll generate a heatmap showing the sentiment occurence as a function of the speakers."
      ],
      "metadata": {
        "id": "ZaGY5ZWBSuEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the chart\n",
        "chart"
      ],
      "metadata": {
        "id": "GgmbvkrbStaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap of Sentiment Analysis 2\n",
        "Let's now zoom into the individual sentences and see the sentiment for sequences of words as spoken in the transcript."
      ],
      "metadata": {
        "id": "-bHZBD7cMSrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "font_size = 12\n",
        "\n",
        "# Define the color scale for sentiment\n",
        "sentiment_colors = {\n",
        "    'POSITIVE': '#4CAF50',  # Green\n",
        "    'NEUTRAL': '#9E9E9E',   # Gray\n",
        "    'NEGATIVE': '#F44336'    # Red\n",
        "}\n",
        "\n",
        "# Create the base chart\n",
        "base = alt.Chart(df).encode(\n",
        "    x=alt.X('speaker:N', axis=alt.Axis(title='Speaker', titleFontSize=font_size, labelFontSize=font_size)),\n",
        "    y=alt.Y('index:O', axis=alt.Axis(title=None, labels=False))  # Use 'index' for Y-axis, hide labels\n",
        ")\n",
        "\n",
        "# Create the heatmap rectangles with black stroke (border)\n",
        "heatmap = base.mark_rect(stroke='black').encode(\n",
        "    color=alt.Color('sentiment:N', scale=alt.Scale(domain=list(sentiment_colors.keys()), range=list(sentiment_colors.values())),\n",
        "                    legend=alt.Legend(orient='bottom')),  # Move legend to the bottom\n",
        "    tooltip=['speaker:N', 'sentiment:N', 'text:N']\n",
        ").properties(\n",
        "    width=200,  # Reduced width for the heatmap\n",
        "    height=df.shape[0] * 20  # Adjust height based on the number of rows\n",
        ")\n",
        "\n",
        "# Add the text column to the left of the chart and hide its y-axis\n",
        "text_right = alt.Chart(df).mark_text(align='left', baseline='middle', dx=5).encode(\n",
        "    y=alt.Y('index:O', axis=None),  # Remove y-axis from text\n",
        "    text=alt.Text('text:N'),\n",
        "    color=alt.value('black')\n",
        ").properties(\n",
        "    width=10,  # Adjust width for the text column\n",
        "    height=df.shape[0] * 20  # Ensure consistent height\n",
        ")\n",
        "\n",
        "# Combine the heatmap and the text\n",
        "chart = alt.concat(\n",
        "    heatmap,\n",
        "    text_right\n",
        ").properties(\n",
        "    # title='Call Center Data Visualization',\n",
        ").configure_axis(\n",
        "    labelFontSize=font_size,\n",
        "    titleFontSize=font_size\n",
        ").configure_view(\n",
        "    strokeOpacity=0\n",
        "    #strokeWidth=1,  # Add a border to the entire view\n",
        "    #stroke='black'  # Make the border black\n",
        ").interactive()\n",
        "\n",
        "chart"
      ],
      "metadata": {
        "id": "2CfLrLgBfXyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "Here are additional resources to dive deeper into topics mentioned in this tutorial.\n",
        "\n",
        "- [🔑 Sign up to get free AssemblyAI API token](https://www.assemblyai.com/?utm_source=youtube&utm_medium=influencer&utm_campaign=dataprofessor&utm_content=apr_25)\n",
        "- [📚 AssemblyAI Documentation](https://www.assemblyai.com/docs/)\n",
        "- [🙂 Sentiment analysis](https://www.assemblyai.com/docs/audio-intelligence/sentiment-analysis)\n",
        "- [👥 Speaker identification](https://www.assemblyai.com/docs/guides/speaker-identification)\n",
        "- [📊 Altair User Guide](https://altair-viz.github.io/user_guide/data.html)"
      ],
      "metadata": {
        "id": "jvxkVdA4VEFb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJPzEGjSndBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
